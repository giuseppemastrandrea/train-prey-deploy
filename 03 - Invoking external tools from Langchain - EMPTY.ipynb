{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8227539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_core.tools import tool\n",
    "from keras.saving import load_model\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a67669-4386-4f6f-b719-a7f9a3fce6ad",
   "metadata": {},
   "source": [
    "### Funzione di Predizione del Rischio di Ictus e Modello ML\n",
    "In questa cella, viene creata una funzione `get_stroke_risk` che utilizza un modello di machine learning per prevedere il rischio di ictus basandosi su dati personali e di salute. La funzione accetta un insieme di variabili (come età, sesso, livello di glucosio medio, BMI, ecc.) e restituisce una predizione binaria (0 o 1) sul rischio di ictus.\n",
    "\n",
    "<a href=\"https://python.langchain.com/v0.2/api_reference/core/tools/langchain_core.tools.convert.tool.html#tool\">tool documentation</a>\n",
    "\n",
    "#### Dettagli del Codice:\n",
    "- **Modello Pydantic**: La classe `DataRow` definisce i campi di input con descrizioni dettagliate per assicurare che i dati siano strutturati correttamente.\n",
    "- **Funzione Decorata con `@tool`**: La funzione `get_stroke_risk` è decorata con `@tool` per poter essere usata come strumento all'interno di un flusso LangChain.\n",
    "- **Caricamento del Modello**: Il modello di rete neurale addestrato viene caricato con `load_model` dalla cartella `model`.\n",
    "- **Predizione**: I dati di input vengono trasformati in un array `tensor` e passati al modello per ottenere la predizione, che è poi confrontata con una soglia (0.4) per generare l'output binario.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Analizza la funzione `get_stroke_risk` per comprendere come vengono utilizzati i dati di input per generare una predizione.\n",
    "- Scrivi il codice per testare la funzione con input diversi e osserva come cambia l'output.\n",
    "- Come esercizio, prova a modificare la soglia di classificazione e analizza come questa modifica influisce sulla sensibilità delle predizioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a60825-ba32-4d80-b160-1478b9b5d0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5caa22-17b1-4564-b762-3aef525e2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invochiamo il tool con .invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cab20-bf96-4e6d-a401-a0eba02cd4bc",
   "metadata": {},
   "source": [
    "### Integrazione degli Strumenti con ToolNode\n",
    "Questa cella crea un nodo `ToolNode` utilizzando strumenti predefiniti per costruire un flusso di lavoro in LangGraph. Gli strumenti `get_weather` e `get_stroke_risk`, definiti in precedenza, vengono inseriti in un'istanza di `ToolNode`, rendendoli utilizzabili all'interno di un flusso complesso.\n",
    "\n",
    "#### Dettagli del Codice:\n",
    "- **`ToolNode`**: Un oggetto di LangGraph che permette di collegare e gestire strumenti all'interno di un flusso di esecuzione.\n",
    "- **Lista di Strumenti**: La variabile `tools` contiene una lista degli strumenti definiti (`get_weather` e `get_stroke_risk`), che vengono passati a `ToolNode` per essere gestiti come parte del grafo.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Crea un `ToolNode` con una lista di strumenti personalizzati e osserva come viene gestito nel flusso di lavoro.\n",
    "- Testa l'uso di `ToolNode` in una sequenza di esecuzione e verifica che gli strumenti funzionino correttamente.\n",
    "- Come esercizio, prova ad aggiungere altri strumenti alla lista e analizza come questi influenzano l'output complessivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07bf80-3a5e-451b-8307-963105bb9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try other tools!\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from langchain_community.tools import WikipediaQueryRun\n",
    "# from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebf0fe4-410d-468d-a156-367c5613857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8f6c6-bb48-4402-98db-e13d71c5e6b4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Costruiamo il grafo con `call_model` e `should_continue`\n",
    "La funzione `should_continue` è responsabile del controllo condizionale per determinare se il flusso di lavoro deve continuare verso il nodo degli strumenti (`tools`) o terminare. Questa logica permette al modello LLM di decidere se invocare strumenti basandosi sull'input dell'utente o sulla risposta generata. \n",
    "\n",
    "- **Presenza di `tool_calls`**: La funzione verifica l'ultimo messaggio nello stato (`last_message`). Se è presente l'attributo `tool_calls`, significa che l'LLM ha deciso di invocare uno strumento e la funzione ritorna `\"tools\"`. In caso contrario, la funzione restituisce `END`, indicando che il flusso deve terminare.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Crea la funzione `should_continue` per includere ulteriori controlli o condizioni personalizzate.\n",
    "- Esegui il flusso di lavoro e osserva come l'LLM decide quando invocare strumenti o terminare l'esecuzione.\n",
    "- Come esercizio, aggiungi log per monitorare quali messaggi contengono `tool_calls` e verifica come ciò influenza l'esecuzione del flusso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6051aa4-a4fd-4dc8-91b2-83ab6892c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    pass\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    pass\n",
    "\n",
    "# workflow = StateGraph(MessagesState)\n",
    "# Build and compile the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de0f8a9-2f61-4497-8b72-b773bc8ad210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ora prova a parlare con il chatbot!\n",
    "\n",
    "# resp['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b7561-09e1-4769-a755-8ba323bb7095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
