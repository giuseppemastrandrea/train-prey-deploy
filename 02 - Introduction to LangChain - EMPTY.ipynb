{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97fef2",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Runnable\n",
    "- Prompt Template\n",
    "- Intro to LangGraph and memory\n",
    "- Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-openai\n",
    "# !pip --version\n",
    "# !pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eeef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE LANGCHAIN VERSION IS 0.2.17\n",
    "\n",
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875316c-72c6-4842-9e6e-8efb1ab0ba6e",
   "metadata": {},
   "source": [
    "### Configurazione del Modello LLM con LangChain e OpenAI\n",
    "Possiamo con langchain e i suoi package configurare un modello di linguaggio (LLM) utilizzando la libreria `langchain_openai` per interagire con i servizi di OpenAI. L'oggetto `ChatOpenAI` consente di impostare i parametri del modello, come la chiave API, la temperatura per controllare la creatività delle risposte, il numero massimo di token e il timeout della richiesta.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per creare un'istanza di `ChatOpenAI` con i parametri forniti.\n",
    "- Assicurati di inserire una chiave API valida al posto di `\"HERE YOUR API KEY\"`.\n",
    "- Come esercizio, prova a modificare i valori di `temperature` e `max_tokens` e osserva come queste modifiche influenzano le risposte del modello.\n",
    "- Invoca il modello chiamando il metodo \"invoke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43c24d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "093e2799-da2a-4fcb-b980-001b1b21d391",
   "metadata": {},
   "source": [
    "### Creazione di un Prompt Template e Collegamento al Modello\n",
    "Si utilizza `ChatPromptTemplate` per creare un template di prompt personalizzato che guida le risposte del modello LLM. In questo esempio, il prompt include un messaggio di sistema che imposta il contesto, chiedendo al modello di rispondere come un ingegnere di Machine Learning di livello mondiale, e di concludere ogni risposta con un riferimento alla bellezza dell'uso della data science nelle decisioni. Il template accetta un input dell'utente tramite un segnaposto `{input}`. Infine, il prompt viene concatenato al modello LLM per creare una catena eseguibile.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per creare un `ChatPromptTemplate` con messaggi personalizzati.\n",
    "- Concatenalo al modello LLM per eseguire la catena di prompt.\n",
    "- Come esercizio, modifica il messaggio di sistema per esplorare come le risposte del modello cambiano in base alle variazioni del contesto fornito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf4d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate  # pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2394085",
   "metadata": {},
   "source": [
    "## Interfaccia Runnable\n",
    "\n",
    "Per semplificare la creazione di catene di eventi/esecuzioni anche molto complesse, tutti i componenti di LangChain implementano un protocollo \"runnable\" attraverso un'interfaccia comune che consente l'utilizzo standard di qualsiasi componente. Di seguito sono riportati i tre metodi principali:\n",
    "\n",
    "* **stream** - invia risposte parziali man mano che vengono generate\n",
    "* **invoke** - esegue la catena su un singolo input\n",
    "* **batch** - esegue la catena su più input\n",
    "\n",
    "### Input e Output dei Componenti Principali\n",
    "<img src=\"assets/componenti_io.png\" width=\"600\">\n",
    "\n",
    "Uno dei vantaggi delle interfacce Runnable è che i componenti eseguibili possono essere collegati insieme in sequenze di esecuzione, permettendo all'output di un componente di diventare automaticamente l'input di un altro. Il comando *pipe* **|** è utilizzato a questo scopo nel LCEL (LangChain Expression Language), consentendo la creazione di componenti eseguibili da altri componenti eseguibili configurandoli in una sequenza che lavorerà in modo sinergico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26d085-9eb1-43aa-ae26-bcc11a63f875",
   "metadata": {},
   "source": [
    "### Invocazione della Catena con un Input\n",
    "In questa cella, si esegue la catena creata passando un input dell'utente al modello. Il metodo `invoke` permette di inviare un messaggio alla catena e ricevere la risposta generata dal modello, seguendo il contesto e le istruzioni definite nel `ChatPromptTemplate`.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per invocare la catena con un input personalizzato e osserva la risposta generata.\n",
    "- Analizza la risposta del modello per verificare se segue le istruzioni del prompt, come terminare con un riferimento alla data science.\n",
    "- Come esercizio, prova a invocare la catena con altri input e osserva come il contesto influisce sulle risposte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c72195-c118-4ce5-a419-8e2c02f051f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec2457-9d26-401b-8ba0-7a18a43cbcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4ecbc",
   "metadata": {},
   "source": [
    "### Flusso di Lavoro per Conversazioni AI con LangChain e LangGraph\n",
    "\n",
    "Questo codice definisce un flusso di lavoro per gestire conversazioni AI utilizzando le librerie **LangChain** e **LangGraph**.\n",
    "\n",
    "#### Creazione del Grafo di Stato\n",
    "Si crea un oggetto `StateGraph` basato su uno schema di stato dei messaggi, che rappresenta il flusso di esecuzione:\n",
    "\n",
    "```python\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "def call_model(state: MessagesState):\n",
    "    ...\n",
    "\n",
    "```\n",
    "#### Funzione per Chiamare il Modello\n",
    "La funzione `call_model`  prepara un prompt iniziale e aggiunge i messaggi provenienti dallo stato. Questi messaggi vengono poi passati al modello per ottenere una risposta.\n",
    "\n",
    "#### Aggiunta di Nodi e Collegamenti\n",
    "Viene aggiunto un nodo chiamato \"model\" al flusso di lavoro e viene creato un collegamento di esecuzione dal nodo di inizio (`START`) al nodo \"model\"\n",
    "\n",
    "#### Checkpointer in Memoria\n",
    "Per salvare e recuperare lo stato della conversazione, viene aggiunto un oggetto `MemorySaver` come `checkpointer`\n",
    "\n",
    "#### Compilazione del Flusso di Lavoro\n",
    "Il flusso di lavoro viene compilato per diventare eseguibile. \n",
    "\n",
    "Questo approccio consente di creare flussi di lavoro modulari per la gestione delle conversazioni AI, con nodi eseguibili e collegamenti che facilitano la gestione dei messaggi e la memoria delle sessioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b39849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad5170-cd60-4be4-91af-6b459ee3c078",
   "metadata": {},
   "source": [
    "### Invocazione del Flusso di Lavoro con Input Personalizzato\n",
    "Questa cella invoca l'applicazione del flusso di lavoro precedentemente compilato, passando un messaggio specifico come input. In questo esempio, il messaggio richiede al modello di tradurre la frase \"I love programming\" in francese. Inoltre, la configurazione include un `thread_id` per tracciare l'esecuzione in un contesto specifico.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per invocare l'applicazione con un input e un contesto configurabile.\n",
    "- Osserva la risposta generata per verificare la correttezza della traduzione e l'aderenza alle istruzioni del prompt.\n",
    "- Come esercizio, prova a modificare l'input e testare diverse frasi o richieste per vedere come il modello gestisce le variazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b61e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836cb54-accf-4755-b6dd-bad8364f7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be7db3c-f6f3-4667-859c-80ff20205838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around and test memory or change threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f3b997-4d6f-4d90-b566-e35b3e45ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for r in resp['messages']:\n",
    "#    print(type(r).__name__, r.content)\n",
    "#    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80bbdd",
   "metadata": {},
   "source": [
    "# Introduzione a PromptTemplate\n",
    "\n",
    "Il `PromptTemplate` è una funzionalità potente progettata per semplificare e standardizzare la creazione di prompt per varie applicazioni, come chatbot, risposte automatiche o moduli di inserimento dati. Fornisce un formato strutturato che può essere riutilizzato in diversi scenari, garantendo coerenza ed efficienza nel modo in cui vengono richiesti e gestiti gli input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5e959-72ce-4ba0-aaf0-cd5ef397990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic template and use of a Memory Buffer\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "conversation = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"question\": \"Hello, i like the orange color.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1197984",
   "metadata": {},
   "source": [
    "## Parsing dell'Output degli LLM\n",
    "\n",
    "<a href=\"https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\" target=\"_blank\">fonte</a>\n",
    "\n",
    "I modelli di linguaggio (LLM) generano testo. Tuttavia, spesso è necessario ottenere informazioni più strutturate rispetto al semplice testo grezzo. È qui che entrano in gioco i parser di output.\n",
    "\n",
    "**I parser di output** sono classi che aiutano a *strutturare le risposte dei modelli di linguaggio*.\n",
    "\n",
    "Un parser di output deve implementare principalmente due metodi:\n",
    "\n",
    "- **\"Get format instructions\"**: Un metodo che restituisce una stringa contenente le istruzioni su come deve essere formattato l'output di un modello di linguaggio.\n",
    "- **\"Parse\"**: Un metodo che accetta una stringa (presumibilmente la risposta di un modello di linguaggio) e la analizza in una struttura.\n",
    "\n",
    "Esiste anche un metodo opzionale:\n",
    "\n",
    "- **\"Parse with prompt\"**: Un metodo che accetta una stringa (presumibilmente la risposta di un modello di linguaggio) e un prompt (presumibilmente il prompt che ha generato tale risposta) e la analizza in una struttura. Il prompt viene fornito principalmente nel caso in cui il parser di output voglia riprovare o correggere l'output in qualche modo, utilizzando le informazioni del prompt per farlo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb65d26-1483-418d-bbfc-ff2b097c9a73",
   "metadata": {},
   "source": [
    "### Assignment: Utilizzo di OutputParser con un Modello LLM\n",
    "In questa cella, si utilizza `OutputParser` per formattare e strutturare le risposte generate da un modello di linguaggio. Si crea un template di prompt che istruisce il modello a rispondere come un data scientist, includendo riferimenti alla bellezza della Data Science. Si utilizza poi `StrOutputParser` per elaborare e formattare l'output del modello.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per creare un `PromptTemplate` con un template personalizzato e concatenalo al modello LLM.\n",
    "- Configura l'`OutputParser` per strutturare l'output del modello.\n",
    "- Esegui la catena di esecuzione con un input di esempio e verifica la formattazione della risposta.\n",
    "- Come esercizio, modifica il template per includere altri dettagli specifici e osserva come cambia la risposta del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe3e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52bb430-4c45-41ae-be54-2bdfb832056b",
   "metadata": {},
   "source": [
    "### Parsing dell'Output con Pydantic e LangChain\n",
    "In questa cella, viene utilizzata la libreria `pydantic` per definire un modello di dati strutturato (`User`) con campi specifici e descrizioni. Questo modello serve a strutturare le risposte in un formato coerente. Successivamente, si utilizza `PydanticOutputParser` di LangChain per creare un parser che formatta le risposte del modello LLM in conformità con il modello `User`.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per definire un modello Pydantic personalizzato e crea un `PydanticOutputParser` basato su di esso.\n",
    "- Stampa le istruzioni di formattazione del parser per capire come il modello LLM dovrebbe strutturare l'output.\n",
    "- Come esercizio, prova a espandere il modello `User` con altri campi (es. `age`, `role`) e osserva come le istruzioni di formattazione cambiano.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee8f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    pass\n",
    "    \n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5f6e2-1a65-490e-afde-b849916c3368",
   "metadata": {},
   "source": [
    "### Creazione di un Prompt con Istruzioni di Formattazione Personalizzate\n",
    "In questa cella, viene creato un `PromptTemplate` che utilizza le istruzioni di formattazione generate dal `PydanticOutputParser`. Il template chiede di analizzare un testo e include le istruzioni di formattazione come parte del prompt. Questo consente di garantire che le risposte del modello LLM siano strutturate in conformità con il modello Pydantic specificato.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per creare un `PromptTemplate` che includa le istruzioni di formattazione del parser.\n",
    "- Verifica che il template sia configurato correttamente e pronto per essere utilizzato in una catena di esecuzione con il modello LLM.\n",
    "- Come esercizio, prova a creare un prompt simile per altri modelli Pydantic e osserva come la formattazione influisce sull'output generato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e8e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a structured description for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db6f408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a more verbose description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b379e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a more verbose description and create a chain to serialize a JSON object\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
