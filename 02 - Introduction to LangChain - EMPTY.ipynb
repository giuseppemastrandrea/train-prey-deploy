{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97fef2",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Runnable\n",
    "- Prompt Template\n",
    "- Sequential memory\n",
    "- Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI  # pip install langchain-openai\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"\", \n",
    "    temperature=.75, \n",
    "    max_tokens=1024, \n",
    "    request_timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbde4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # pip install langchain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act as a world class Machine Learning engineer. Use english language. End your answers with a reference to the beauty of using data science in any decision you make.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2394085",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "\n",
    "To simplify the creation of even very complex event/execution chains, all LangChain components implement a \"runnable\" protocol through a common interface that allows any component to be used in a standard way. Below are the three main methods:\n",
    "\n",
    "* **stream** - send partial responses as they are generated\n",
    "* **invoke** - execute the chain on a single input\n",
    "* **batch** - execute the chain on multiple inputs\n",
    "\n",
    "### Input and Output of Main Components\n",
    "<img src=\"assets/componenti_io.png\" width=\"600\">\n",
    "\n",
    "One of the advantages of Runnable interfaces is that runnable components can be chained together in execution sequences, allowing the output of one component to automatically become the input to another. The *pipe* command **|** is used for this purpose in LCEL (LangChain Expression Language), enabling the creation of runnable components from other runnable components by configuring them into a sequence that will work synergistically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69054c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"hello!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4ecbc",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory\n",
    "\n",
    "[`ConversationBufferMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) is a tool in LangChain that helps keep track of a conversation. It stores the messages exchanged between the user and the AI so that the AI can remember what has been said earlier. This helps the AI maintain context and continuity in the conversation.\n",
    "\n",
    "`ConversationBufferMemory` is a type of sequential memory in Langchain:\n",
    "\n",
    "<img src=\"assets/sequential-memory.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "Hereâ€™s a basic example of how to add messages to a `ConversationBufferMemory` and how to get back the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b39849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a new conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add user and AI messages to the memory\n",
    "memory.chat_memory.add_user_message(\"Hello\")\n",
    "memory.chat_memory.add_ai_message(\"Hi! How you doin'?\")\n",
    "memory.chat_memory.add_user_message(\"Fine, thanks.\")\n",
    "\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b61e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Add user and AI messages to the memory\n",
    "memory.chat_memory.add_user_message(\"Hello\")\n",
    "memory.chat_memory.add_ai_message(\"Hi! How you doin'?\")\n",
    "memory.chat_memory.add_user_message(\"Fine, thanks.\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80bbdd",
   "metadata": {},
   "source": [
    "# Introduction to PromptTemplate\n",
    "\n",
    "The `PromptTemplate` is a powerful feature designed to streamline and standardize the creation of prompts for various applications, such as chatbots, automated responses, or data entry forms. It provides a structured format that can be reused across different scenarios, ensuring consistency and efficiency in how inputs are solicited and processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98116ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic template and use of a Memory Buffer\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "Conversation:\n",
    "{chat}\n",
    "\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"question\": \"Hello, i lake the orange color.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.load_memory_variables({})['chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9961a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"question\": \"Tell me 3 fruits of my favourite color\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33227adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.load_memory_variables({})['chat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1197984",
   "metadata": {},
   "source": [
    "## LLM output parsing\n",
    "\n",
    "<a href=\"https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/quick_start/\" target=\"_blank\">source</a>\n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
    "\n",
    "**Output parsers** are classes that help *structure language model responses*. \n",
    "\n",
    "There are two main methods an output parser must implement:\n",
    "\n",
    "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full chain with a prompt, a model and an output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(description=\"user identification number\")\n",
    "    name: str = Field(description=\"user name\")\n",
    "    mail: str = Field(description=\"user mail address\")\n",
    "    \n",
    "\n",
    "# create a chain with a Pydantic object parser\n",
    "# create a chain with a JsonOutputParser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
