{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U imbalanced-learn -> Not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a5ace-3691-4233-9be1-2ff9a614221d",
   "metadata": {},
   "source": [
    "### Import delle Librerie e Setup\n",
    "Questa cella importa tutte le librerie necessarie per l'analisi, la pre-elaborazione dei dati e la costruzione di modelli di machine learning. Tra le librerie importate ci sono:\n",
    "- **pandas, numpy, seaborn, matplotlib** per la manipolazione dei dati e la visualizzazione.\n",
    "- **scikit-learn** per la divisione dei dati in set di addestramento e test, la valutazione dei modelli, l'imputazione di valori mancanti e la normalizzazione.\n",
    "- **TensorFlow/Keras** per la costruzione e l'addestramento del modello di rete neurale.\n",
    "- Configurazioni di matplotlib per migliorare la qualità delle immagini.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Verificate che tutte le librerie siano correttamente installate sul vostro ambiente eseguendo questa cella.\n",
    "- Se necessario, installate eventuali librerie mancanti usando `!pip install <nome_libreria>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc946847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common libraries\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Data preprocessing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, log_loss, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Libraries to build our model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "# matplotlib configs to show decent images\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79947459-bc1c-4741-bf7c-044230ef1484",
   "metadata": {},
   "source": [
    "### Caricamento del Dataset e Esplorazione Iniziale\n",
    "Questa cella carica il dataset principale relativo ai dati sanitari sui casi di ictus utilizzando `pandas`. L'indice del DataFrame è impostato sulla colonna \"id\" per identificare univocamente ogni riga. Dopo il caricamento, la cella stampa la dimensione del dataset e fornisce una panoramica delle colonne con `df.info()`.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Eseguite la cella per caricare il dataset e osservare la sua struttura.\n",
    "- Annotate il numero di righe e colonne e verificate la presenza di eventuali valori nulli o tipi di dati non corretti.\n",
    "- Come esercizio, provate a scrivere un comando per visualizzare le prime 5 righe del dataset e discutete le caratteristiche principali delle variabili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1d12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b63d80-d925-4d6a-b1a1-7a909258de9e",
   "metadata": {},
   "source": [
    "### Reindicizzazione del DataFrame e Visualizzazione Iniziale\n",
    "C'è bisogno di reindicizzare il DataFrame rimuovendo la colonna \"id\" come indice e utilizzando un nuovo indice numerico sequenziale, rendendo più agevole l'accesso alle righe. Successivamente, si utilizza il metodo `df.head()` per visualizzare le prime cinque righe del dataset e ottenere un'anteprima dei dati.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per reindicizzare il DataFrame e visualizzare le prime righe.\n",
    "- Controllate la coerenza dei dati e verificate se ci sono valori anomali o formati non corretti.\n",
    "- Come esercizio, provate a visualizzare informazioni statistiche di base sulle colonne numeriche utilizzando `df.describe()` per avere un'idea della distribuzione dei dati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7b2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c074e6b-97ef-4836-8914-665d23e0a397",
   "metadata": {},
   "source": [
    "### Verifica dei Valori Unici per le Colonne di Tipo Oggetto\n",
    "C'è bisogno di controllare i valori unici presenti nelle colonne di tipo `object` per comprendere meglio la distribuzione delle categorie e rilevare eventuali valori anomali o incongruenze. Questa operazione aiuta a preparare i dati per una successiva codifica o analisi.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per stampare i valori unici di ciascuna colonna categoriale del DataFrame.\n",
    "- Osserva i valori unici per identificare eventuali anomalie o categorie non standard che potrebbero richiedere una pulizia o un trattamento specifico.\n",
    "- Come esercizio, prova a sostituire eventuali valori mancanti o anomali con una categoria standard o un valore adeguato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ca89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for object dtype columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1cb894",
   "metadata": {},
   "source": [
    "### Imputazione dei Valori Mancanti nella Colonna `bmi`\n",
    "Per gestire i valori mancanti nella colonna `bmi`, è utile applicare l'imputazione utilizzando la strategia della media. In questo caso, si salva l'imputer come file `.sav` per poterlo riutilizzare in futuro. L'imputer viene adattato alla colonna `bmi` e applicato per sostituire i valori mancanti con la media calcolata.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per creare un oggetto `SimpleImputer` con la strategia della media e salvarlo come file per un uso successivo.\n",
    "- Applica l'imputer alla colonna `bmi` per sostituire i valori mancanti e verifica il risultato.\n",
    "- Come esercizio, prova a utilizzare una strategia diversa per l'imputazione (ad esempio, `median`) e confronta i risultati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e912cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Missing values before imputation:\", df['bmi'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1acdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Missing values after imputation:\", df['bmi'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79a16c-c7fb-4646-8e36-e0694754a7f1",
   "metadata": {},
   "source": [
    "### Codifica delle Colonne Categoriali con One-Hot Encoding\n",
    "Per poter utilizzare le colonne categoriali in modelli di machine learning, è essenziale trasformarle in un formato numerico. In questo caso, si usa `OneHotEncoder` per effettuare la codifica one-hot delle colonne di tipo `object`, creando nuove colonne binarie per ciascuna categoria unica. L'encoder è poi salvato come file `.sav` per un riutilizzo futuro.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per selezionare le colonne categoriali e applicare la codifica one-hot.\n",
    "- Salva l'encoder addestrato in un file per usi futuri e stampa un'anteprima delle colonne codificate.\n",
    "- Come esercizio, prova ad analizzare la dimensione del nuovo DataFrame con le colonne codificate e confronta il numero di colonne prima e dopo la codifica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn OneHotEncoder to encode the categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e954734-fe91-4d66-b735-594750841c71",
   "metadata": {},
   "source": [
    "### Aggiunta delle Colonne Codificate e Aggiornamento del DataFrame\n",
    "Per incorporare le colonne codificate nel DataFrame originale, è necessario concatenare il DataFrame esistente con il DataFrame delle colonne codificate. Successivamente, si eliminano le colonne originali categoriali, poiché sono state sostituite dalle nuove colonne codificate. Infine, si aggiorna il DataFrame principale con il nuovo contenuto.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per unire il DataFrame originale con le colonne codificate e rimuovere le colonne categoriali originali.\n",
    "- Assicurati che il DataFrame finale sia coerente e pronto per l'analisi successiva.\n",
    "- Come esercizio, verifica il numero di colonne nel nuovo DataFrame e controlla che non ci siano duplicati o incongruenze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a6deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed32df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4798581-b78c-46fe-bd91-c83a430952d0",
   "metadata": {},
   "source": [
    "### Normalizzazione e Visualizzazione delle Colonne Numeriche\n",
    "È importante normalizzare le colonne numeriche per garantire che i dati siano su una scala simile, migliorando così le prestazioni dei modelli. Questa cella identifica le colonne numeriche selezionate, visualizza la loro distribuzione tramite boxplot, applica la standardizzazione usando `StandardScaler`, e salva lo scaler per un uso futuro. Infine, viene stampato il DataFrame aggiornato e viene visualizzata la nuova distribuzione dei dati normalizzati.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per visualizzare i boxplot delle colonne numeriche prima e dopo la normalizzazione.\n",
    "- Applica la standardizzazione alle colonne numeriche selezionate e salva lo scaler in un file per il riutilizzo.\n",
    "- Come esercizio, confronta i grafici per osservare come la distribuzione dei dati cambia dopo la normalizzazione e discuti le implicazioni per il modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d099af-fd95-48d3-bb0f-7e7ae74953cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a boxplot of the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e00008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numnerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5b40a-a8f6-4306-aa43-86d95eae6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a boxplot of the numerical features after the standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707febe3",
   "metadata": {},
   "source": [
    "### Rimozione degli Outlier Utilizzando l'IQR\n",
    "Per migliorare la qualità dei dati e ridurre l'influenza degli outlier sulle analisi e sui modelli, si applica il metodo dell'Interquartile Range (IQR). Questa cella calcola l'IQR per ciascuna colonna numerica selezionata, determina i limiti superiori e inferiori per identificare gli outlier, e rimuove le righe che contengono valori al di fuori di questi limiti. Infine, viene stampata la nuova dimensione del DataFrame aggiornato.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per calcolare l'IQR per ciascuna colonna numerica e identificare i limiti degli outlier.\n",
    "- Rimuovi gli outlier e controlla come cambia la dimensione del DataFrame.\n",
    "- Come esercizio, prova a variare la soglia di `outlier_threshold` e osserva come questa modifica influisce sul numero di righe eliminate e sulla distribuzione dei dati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use IQR to delete the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a boxplot of the data after the IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3249bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(df_encoded[float_columns], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fc335-0016-4250-9bd2-bf4a4e8ced63",
   "metadata": {},
   "source": [
    "### Visualizzazione della Distribuzione della Variabile Target\n",
    "Per comprendere la distribuzione della variabile target `stroke`, si utilizza una tabella pivot per contare il numero di occorrenze di ciascuna classe e si visualizza il risultato sotto forma di grafico a barre. Questa analisi è utile per identificare eventuali squilibri nel dataset, che potrebbero richiedere tecniche di bilanciamento o adattamenti specifici durante l'addestramento del modello.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per creare una tabella pivot che mostri la distribuzione della variabile target e visualizza un grafico a barre per una comprensione rapida della distribuzione.\n",
    "- Osserva se ci sono squilibri significativi nelle classi e discuti le possibili strategie per gestirli.\n",
    "- Come esercizio, prova a calcolare anche la percentuale di distribuzione per ciascuna classe e visualizzala per un'analisi più dettagliata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369358b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_fieldname = \"stroke\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c598d54-0740-42ab-ac0d-3819b340055f",
   "metadata": {},
   "source": [
    "### Suddivisione del Dataset in Set di Addestramento e Test\n",
    "Per preparare il dataset all'addestramento del modello, è necessario suddividerlo in set di addestramento e di test. Si utilizza la funzione `train_test_split` con stratificazione basata sulla variabile target `stroke` per mantenere la stessa distribuzione delle classi in entrambi i set. Successivamente, si rimuove la colonna della variabile target dai set di input.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per suddividere il dataset in set di addestramento e di test, mantenendo la stratificazione sulla variabile target.\n",
    "- Rimuovi la colonna `stroke` dai set di input e verifica la dimensione dei set creati.\n",
    "- Come esercizio, prova a modificare il parametro `test_size` per esplorare diverse proporzioni di addestramento/test e discuti l'impatto sui risultati del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e12d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.pivot_table(index=y_fieldname, aggfunc=\"size\").plot(kind='bar', title=\"Distribuzione\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9910664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.pivot_table(index=y_fieldname, aggfunc=\"size\").plot(kind='bar', title=\"Distribuzione\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b7f91-ba90-4936-b045-26bd574e4110",
   "metadata": {},
   "source": [
    "### Ripetizione del Preprocessing: Imputazione, Codifica One-Hot e Standardizzazione\n",
    "Dopo aver capito come funzionano questi strumenti, rifacciamo tutto da capo! Per garantire un processo corretto di pre-elaborazione dei dati, dobbiamo ripetere l'imputazione dei valori mancanti e la codifica one-hot eseguendo `fit` solo sul dataset di train e applicando `transform` sia sul dataset di train che su quello di test. Lo stesso principio si applica alla standardizzazione, in cui il metodo `fit_transform` viene usato sul set di addestramento e `transform` sul set di test. Questo processo assicura che il set di test rimanga separato dal set di train, evitando il data leakage.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per eseguire l'imputazione e la codifica one-hot applicando `fit` sul set di train e `transform` sia sul set di train che sul set di test.\n",
    "- Ripeti la standardizzazione con `fit_transform` sul set di train e `transform` sul set di test.\n",
    "- Come esercizio, verifica che i set di train e test abbiano subito le stesse trasformazioni e siano coerenti per l'addestramento del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e8b92-ccb6-42a3-a71f-ac6487332a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ac11c8-1c59-45b7-bdf9-61bcf7cbc857",
   "metadata": {},
   "source": [
    "### Creazione, Addestramento e Valutazione del Modello di Rete Neurale\n",
    "In questa cella, si costruisce un modello di rete neurale sequenziale con due livelli densi e dropout per ridurre l'overfitting. Il modello è compilato utilizzando l'ottimizzatore Adam, la funzione di perdita `binary_crossentropy`, e l'AUC come metrica. I pesi di classe sono calcolati per bilanciare le classi nel dataset di training, e il modello viene addestrato con i dati di train e validato sui dati di test. Infine, si fanno predizioni sul set di test e si visualizza la matrice di confusione per valutare le prestazioni del modello.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per costruire un modello di rete neurale sequenziale con livelli densi e dropout.\n",
    "- Compila il modello utilizzando una metrica di tua scelta e addestralo usando i set di training e test.\n",
    "- Come esercizio, prova a variare l'architettura del modello (numero di livelli e neuroni) e osserva come cambiano le prestazioni. Visualizza la matrice di confusione per valutare i risultati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba07f32-5293-4583-8a5f-8564a70ab53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e6ba36e-3e82-4a9a-8c3f-6a58a4bffc0e",
   "metadata": {},
   "source": [
    "### Calcolo e Visualizzazione della Matrice di Confusione\n",
    "Per valutare le prestazioni del modello, è necessario calcolare e visualizzare la matrice di confusione. Questa matrice mostra il numero di predizioni corrette e incorrette per ciascuna classe, aiutando a comprendere meglio la precisione e il bilanciamento del modello. Una visualizzazione grafica della matrice facilita l'interpretazione dei risultati.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per calcolare e visualizzare la matrice di confusione per il set di test.\n",
    "- Osserva i risultati della matrice per identificare il numero di falsi positivi, falsi negativi, veri positivi e veri negativi.\n",
    "- Come esercizio, prova a variare la soglia di classificazione e confronta come questo influisce sulla matrice di confusione e sulle metriche del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af7064-bbf0-4ffd-a9a9-d2687716e445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8726d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9447c-1064-4bb4-a784-7326baa6ff59",
   "metadata": {},
   "source": [
    "### Salvataggio del Modello Addestrato\n",
    "Dopo aver addestrato il modello, è importante salvarlo per un uso futuro senza dover ripetere l'addestramento. Questa cella salva il modello addestrato in un file `.keras`, consentendo di caricarlo e utilizzarlo successivamente per predizioni o ulteriori valutazioni.\n",
    "\n",
    "### Assignment per i Corsisti\n",
    "- Scrivi il codice per salvare il modello addestrato in un file con estensione `.keras`.\n",
    "- Verifica che il file sia stato salvato correttamente nella directory specificata.\n",
    "- Come esercizio, prova a scrivere il codice per caricare il modello salvato e utilizzarlo per fare nuove predizioni su un insieme di dati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3585e5-5a98-432d-86ba-4b4ab820fe85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
