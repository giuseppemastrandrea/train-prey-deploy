{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97fef2",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Runnable\n",
    "- Prompt Template\n",
    "- Intro to LangGraph and memory\n",
    "- Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-openai\n",
    "# !pip --version\n",
    "# !pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eeef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE LANGCHAIN VERSION IS 0.2.17\n",
    "\n",
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI  # pip install langchain-openai\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"HERE YOUR API KEY\", \n",
    "    temperature=.75, \n",
    "    max_tokens=1024, \n",
    "    request_timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbde4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # pip install langchain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act as a world class Machine Learning engineer. Use english language. End your answers with a reference to the beauty of using data science in any decision you make.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2394085",
   "metadata": {},
   "source": [
    "## Interfaccia Runnable\n",
    "\n",
    "Per semplificare la creazione di catene di eventi/esecuzioni anche molto complesse, tutti i componenti di LangChain implementano un protocollo \"runnable\" attraverso un'interfaccia comune che consente l'utilizzo standard di qualsiasi componente. Di seguito sono riportati i tre metodi principali:\n",
    "\n",
    "* **stream** - invia risposte parziali man mano che vengono generate\n",
    "* **invoke** - esegue la catena su un singolo input\n",
    "* **batch** - esegue la catena su più input\n",
    "\n",
    "### Input e Output dei Componenti Principali\n",
    "<img src=\"assets/componenti_io.png\" width=\"600\">\n",
    "\n",
    "Uno dei vantaggi delle interfacce Runnable è che i componenti eseguibili possono essere collegati insieme in sequenze di esecuzione, permettendo all'output di un componente di diventare automaticamente l'input di un altro. Il comando *pipe* **|** è utilizzato a questo scopo nel LCEL (LangChain Expression Language), consentendo la creazione di componenti eseguibili da altri componenti eseguibili configurandoli in una sequenza che lavorerà in modo sinergico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69054c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"hello!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec2457-9d26-401b-8ba0-7a18a43cbcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4ecbc",
   "metadata": {},
   "source": [
    "### Flusso di Lavoro per Conversazioni AI con LangChain e LangGraph\n",
    "\n",
    "Questo codice definisce un flusso di lavoro per gestire conversazioni AI utilizzando le librerie **LangChain** e **LangGraph**.\n",
    "\n",
    "#### Creazione del Grafo di Stato\n",
    "Si crea un oggetto `StateGraph` basato su uno schema di stato dei messaggi, che rappresenta il flusso di esecuzione:\n",
    "\n",
    "```python\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "def call_model(state: MessagesState):\n",
    "    ...\n",
    "\n",
    "```\n",
    "#### Funzione per Chiamare il Modello\n",
    "La funzione `call_model`  prepara un prompt iniziale e aggiunge i messaggi provenienti dallo stato. Questi messaggi vengono poi passati al modello per ottenere una risposta.\n",
    "\n",
    "#### Aggiunta di Nodi e Collegamenti\n",
    "Viene aggiunto un nodo chiamato \"model\" al flusso di lavoro e viene creato un collegamento di esecuzione dal nodo di inizio (`START`) al nodo \"model\"\n",
    "\n",
    "#### Checkpointer in Memoria\n",
    "Per salvare e recuperare lo stato della conversazione, viene aggiunto un oggetto `MemorySaver` come `checkpointer`\n",
    "\n",
    "#### Compilazione del Flusso di Lavoro\n",
    "Il flusso di lavoro viene compilato per diventare eseguibile. \n",
    "\n",
    "Questo approccio consente di creare flussi di lavoro modulari per la gestione delle conversazioni AI, con nodi eseguibili e collegamenti che facilitano la gestione dei messaggi e la memoria delle sessioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b39849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = chain.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b61e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836cb54-accf-4755-b6dd-bad8364f7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7db3c-f6f3-4667-859c-80ff20205838",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"what did i just say?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3b997-4d6f-4d90-b566-e35b3e45ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in resp['messages']:\n",
    "    print(type(r).__name__, r.content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80bbdd",
   "metadata": {},
   "source": [
    "# Introduzione a PromptTemplate\n",
    "\n",
    "Il `PromptTemplate` è una funzionalità potente progettata per semplificare e standardizzare la creazione di prompt per varie applicazioni, come chatbot, risposte automatiche o moduli di inserimento dati. Fornisce un formato strutturato che può essere riutilizzato in diversi scenari, garantendo coerenza ed efficienza nel modo in cui vengono richiesti e gestiti gli input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98116ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic template and use of a Memory Buffer\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "conversation = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"question\": \"Hello, i like the orange color.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1197984",
   "metadata": {},
   "source": [
    "## LLM output parsing\n",
    "\n",
    "<a href=\"https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/quick_start/\" target=\"_blank\">source</a>\n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
    "\n",
    "**Output parsers** are classes that help *structure language model responses*. \n",
    "\n",
    "There are two main methods an output parser must implement:\n",
    "\n",
    "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutputParser\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"Mi piace il colore arancione\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(description=\"user identification number\")\n",
    "    name: str = Field(description=\"user name\")\n",
    "    mail: str = Field(description=\"user mail address\")\n",
    "    \n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=User)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Analyze this text\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "query = \"id:1, name: John Doe, e-mail: john.doe@johndoe.com\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "query = \"my name is giuseppe mastrandrea, my email is mastrandreagiuseppe@gmail.com and my id is 500\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b379e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser(pydantic_object=User)\n",
    "\n",
    "chain = prompt | llm | json_parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
