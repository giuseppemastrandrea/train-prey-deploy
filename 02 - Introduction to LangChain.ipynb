{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97fef2",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Runnable\n",
    "- Prompt Template\n",
    "- Sequential memory\n",
    "- Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befc5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (0.1.20)\n",
      "Requirement already satisfied: langchain-openai in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (0.1.6)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.1.57)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain-openai) (1.28.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Collecting typing-extensions<5,>=4.7\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: sniffio in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.3)\n",
      "Requirement already satisfied: certifi in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2022.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.5.0\n",
      "    Uninstalling typing-extensions-4.5.0:\n",
      "      Successfully uninstalled typing-extensions-4.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\u001b[0m\n",
      "Successfully installed typing-extensions-4.11.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/giumast/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43c24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI  # pip install langchain-openai\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"sk-proj-ux2f5nFSFFuOIQvlwJbbT3BlbkFJuu1vCCWysrlGwKay3oGi\", \n",
    "    temperature=.75, \n",
    "    max_tokens=1024, \n",
    "    request_timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbde4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3a09b429-4efe-450d-80b8-004264ba0f98-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # pip install langchain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act as a world class Machine Learning engineer. Use english language. End your answers with a reference to the beauty of using data science in any decision you make.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2394085",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "\n",
    "To simplify the creation of even very complex event/execution chains, all LangChain components implement a \"runnable\" protocol through a common interface that allows any component to be used in a standard way. Below are the three main methods:\n",
    "\n",
    "* **stream** - send partial responses as they are generated\n",
    "* **invoke** - execute the chain on a single input\n",
    "* **batch** - execute the chain on multiple inputs\n",
    "\n",
    "### Input and Output of Main Components\n",
    "<img src=\"assets/componenti_io.png\" width=\"600\">\n",
    "\n",
    "One of the advantages of Runnable interfaces is that runnable components can be chained together in execution sequences, allowing the output of one component to automatically become the input to another. The *pipe* command **|** is used for this purpose in LCEL (LangChain Expression Language), enabling the creation of runnable components from other runnable components by configuring them into a sequence that will work synergistically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69054c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"hello!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4ecbc",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory\n",
    "\n",
    "[`ConversationBufferMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) is a tool in LangChain that helps keep track of a conversation. It stores the messages exchanged between the user and the AI so that the AI can remember what has been said earlier. This helps the AI maintain context and continuity in the conversation.\n",
    "\n",
    "`ConversationBufferMemory` is a type of sequential memory in Langchain:\n",
    "\n",
    "<img src=\"assets/sequential-memory.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "Here’s a basic example of how to add messages to a `ConversationBufferMemory` and how to get back the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b39849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello\n",
      "AI: Hi! How you doin'?\n",
      "Human: Fine, thanks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a new conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add user and AI messages to the memory\n",
    "memory.chat_memory.add_user_message(\"Hello\")\n",
    "memory.chat_memory.add_ai_message(\"Hi! How you doin'?\")\n",
    "memory.chat_memory.add_user_message(\"Fine, thanks.\")\n",
    "\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b61e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hello'),\n",
       "  AIMessage(content=\"Hi! How you doin'?\"),\n",
       "  HumanMessage(content='Fine, thanks.')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Add user and AI messages to the memory\n",
    "memory.chat_memory.add_user_message(\"Hello\")\n",
    "memory.chat_memory.add_ai_message(\"Hi! How you doin'?\")\n",
    "memory.chat_memory.add_user_message(\"Fine, thanks.\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80bbdd",
   "metadata": {},
   "source": [
    "# Introduction to PromptTemplate\n",
    "\n",
    "The `PromptTemplate` is a powerful feature designed to streamline and standardize the creation of prompts for various applications, such as chatbots, automated responses, or data entry forms. It provides a structured format that can be reused across different scenarios, ensuring consistency and efficiency in how inputs are solicited and processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98116ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giumast/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# dynamic template and use of a Memory Buffer\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "Conversation:\n",
    "{chat}\n",
    "\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43fc824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAct as a data scientist answering to every question with references to the beauty of Data Science.\n",
      "Conversation:\n",
      "\n",
      "\n",
      "New question: Hello, i lake the orange color.\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello, i lake the orange color.',\n",
       " 'chat': '',\n",
       " 'text': 'Orange is a beautiful color, just like the beauty of data science lies in its ability to uncover valuable insights and patterns from large amounts of data. Just as the color orange can evoke feelings of warmth and energy, data science can bring to light new perspectives and understanding of complex phenomena. It truly is a fascinating field that continues to amaze and inspire.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"Hello, i lake the orange color.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08fba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, i lake the orange color.\n",
      "AI: Orange is a beautiful color, just like the beauty of data science lies in its ability to uncover valuable insights and patterns from large amounts of data. Just as the color orange can evoke feelings of warmth and energy, data science can bring to light new perspectives and understanding of complex phenomena. It truly is a fascinating field that continues to amaze and inspire.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9961a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAct as a data scientist answering to every question with references to the beauty of Data Science.\n",
      "Conversation:\n",
      "Human: Hello, i lake the orange color.\n",
      "AI: Orange is a beautiful color, just like the beauty of data science lies in its ability to uncover valuable insights and patterns from large amounts of data. Just as the color orange can evoke feelings of warmth and energy, data science can bring to light new perspectives and understanding of complex phenomena. It truly is a fascinating field that continues to amaze and inspire.\n",
      "\n",
      "New question: Tell me 3 fruits of my favourite color\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Tell me 3 fruits of my favourite color',\n",
       " 'chat': 'Human: Hello, i lake the orange color.\\nAI: Orange is a beautiful color, just like the beauty of data science lies in its ability to uncover valuable insights and patterns from large amounts of data. Just as the color orange can evoke feelings of warmth and energy, data science can bring to light new perspectives and understanding of complex phenomena. It truly is a fascinating field that continues to amaze and inspire.',\n",
       " 'text': 'Three fruits of your favorite color, orange, are oranges, peaches, and apricots. Just like the vibrant hue of these fruits, data science brings a colorful array of insights and discoveries to light, making it an exciting and dynamic field to explore. The beauty of data science is in its ability to showcase the richness and diversity of information that can be gleaned from analyzing data.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"Tell me 3 fruits of my favourite color\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33227adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.load_memory_variables({})['chat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1197984",
   "metadata": {},
   "source": [
    "## LLM output parsing\n",
    "\n",
    "<a href=\"https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/quick_start/\" target=\"_blank\">source</a>\n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
    "\n",
    "**Output parsers** are classes that help *structure language model responses*. \n",
    "\n",
    "There are two main methods an output parser must implement:\n",
    "\n",
    "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe3e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Il colore arancione è davvero affascinante, proprio come il mondo della scienza dei dati! Attraverso l'analisi dei dati, possiamo scoprire tendenze nascoste, modelli interessanti e informazioni preziose che possono portare a nuove scoperte e innovazioni. La bellezza della scienza dei dati risiede nella sua capacità di trasformare dati grezzi in conoscenza significativa, aprendo nuove prospettive e possibilità.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OutputParser\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"Mi piace il colore arancione\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee8f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"id\": {\"title\": \"Id\", \"description\": \"user identification number\", \"type\": \"integer\"}, \"name\": {\"title\": \"Name\", \"description\": \"user name\", \"type\": \"string\"}, \"mail\": {\"title\": \"Mail\", \"description\": \"user mail address\", \"type\": \"string\"}}, \"required\": [\"id\", \"name\", \"mail\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(description=\"user identification number\")\n",
    "    name: str = Field(description=\"user name\")\n",
    "    mail: str = Field(description=\"user mail address\")\n",
    "    \n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=User)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d9fbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"id\": {\"title\": \"Id\", \"description\": \"user identification number\", \"type\": \"integer\"}, \"name\": {\"title\": \"Name\", \"description\": \"user name\", \"type\": \"string\"}, \"mail\": {\"title\": \"Mail\", \"description\": \"user mail address\", \"type\": \"string\"}}, \"required\": [\"id\", \"name\", \"mail\"]}\\n```'}, template='Analyze this text\\n{format_instructions}\\n{query}\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Analyze this text\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e8e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=1, name='John Doe', mail='john.doe@johndoe.com')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "query = \"id:1, name: John Doe, e-mail: john.doe@johndoe.com\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db6f408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=500, name='giuseppe mastrandrea', mail='mastrandreagiuseppe@gmail.com')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "query = \"my name is giuseppe mastrandrea, my email is mastrandreagiuseppe@gmail.com and my id is 500\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b379e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 500,\n",
       " 'name': 'giuseppe mastrandrea',\n",
       " 'mail': 'mastrandreagiuseppe@gmail.com'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser(pydantic_object=User)\n",
    "\n",
    "chain = prompt | llm | json_parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
