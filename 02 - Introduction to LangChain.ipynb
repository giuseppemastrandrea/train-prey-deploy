{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e97fef2",
   "metadata": {},
   "source": [
    "## Topics\n",
    "\n",
    "- Runnable\n",
    "- Prompt Template\n",
    "- Sequential memory\n",
    "- Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befc5d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.60-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from langchain) (1.22.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from langchain) (8.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0\n",
      "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-macosx_11_0_arm64.whl (389 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from langchain) (2.0.29)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0\n",
      "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from langchain) (2.28.1)\n",
      "Collecting openai<2.0.0,>=1.24.0\n",
      "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-macosx_11_0_arm64.whl (906 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.8/906.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.10.3-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (253 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.7 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.9.0)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m328.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.64.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2022.9.13)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: sniffio, packaging, orjson, mypy-extensions, multidict, jsonpointer, h11, frozenlist, exceptiongroup, distro, async-timeout, yarl, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, anyio, aiosignal, langsmith, httpx, dataclasses-json, aiohttp, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.3.3 which is incompatible.\n",
      "tensorflow-macos 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 anyio-4.3.0 async-timeout-4.0.3 dataclasses-json-0.6.6 distro-1.9.0 exceptiongroup-1.2.1 frozenlist-1.4.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.0 langchain-core-0.2.1 langchain-openai-0.1.7 langchain-text-splitters-0.2.0 langsmith-0.1.60 marshmallow-3.21.2 multidict-6.0.5 mypy-extensions-1.0.0 openai-1.30.1 orjson-3.10.3 packaging-23.2 sniffio-1.3.1 tiktoken-0.7.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43c24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI  # pip install langchain-openai\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"\", \n",
    "    temperature=.75, \n",
    "    max_tokens=1024, \n",
    "    request_timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbde4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5f33b47f-269d-45f8-aea3-63b1a7671071-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf4d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # pip install langchain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Act as a world class Machine Learning engineer. Use english language. End your answers with a reference to the beauty of using data science in any decision you make.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# concatenazione del prompt al modello\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2394085",
   "metadata": {},
   "source": [
    "## Runnable Interface\n",
    "\n",
    "To simplify the creation of even very complex event/execution chains, all LangChain components implement a \"runnable\" protocol through a common interface that allows any component to be used in a standard way. Below are the three main methods:\n",
    "\n",
    "* **stream** - send partial responses as they are generated\n",
    "* **invoke** - execute the chain on a single input\n",
    "* **batch** - execute the chain on multiple inputs\n",
    "\n",
    "### Input and Output of Main Components\n",
    "<img src=\"assets/componenti_io.png\" width=\"600\">\n",
    "\n",
    "One of the advantages of Runnable interfaces is that runnable components can be chained together in execution sequences, allowing the output of one component to automatically become the input to another. The *pipe* command **|** is used for this purpose in LCEL (LangChain Expression Language), enabling the creation of runnable components from other runnable components by configuring them into a sequence that will work synergistically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69054c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 45, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d89d5046-5052-44c6-b709-ca6d8c185289-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4ecbc",
   "metadata": {},
   "source": [
    "# ConversationBufferMemory\n",
    "\n",
    "[`ConversationBufferMemory`](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) is a tool in LangChain that helps keep track of a conversation. It stores the messages exchanged between the user and the AI so that the AI can remember what has been said earlier. This helps the AI maintain context and continuity in the conversation.\n",
    "\n",
    "`ConversationBufferMemory` is a type of sequential memory in Langchain:\n",
    "\n",
    "<img src=\"assets/sequential-memory.png\" width=\"300\" />\n",
    "\n",
    "\n",
    "Here’s a basic example of how to add messages to a `ConversationBufferMemory` and how to get back the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b39849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello\n",
      "AI: Hi! How you doin'?\n",
      "Human: Fine, thanks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a new conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Add user and AI messages to the memory\n",
    "memory.chat_memory.add_user_message(\"Hello\")\n",
    "memory.chat_memory.add_ai_message(\"Hi! How you doin'?\")\n",
    "memory.chat_memory.add_user_message(\"Fine, thanks.\")\n",
    "\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76b61e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hello'),\n",
       "  AIMessage(content=\"Hi! How you doin'?\"),\n",
       "  HumanMessage(content='Fine, thanks.')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# Add user and AI messages to the memory\n",
    "memory.chat_memory.add_user_message(\"Hello\")\n",
    "memory.chat_memory.add_ai_message(\"Hi! How you doin'?\")\n",
    "memory.chat_memory.add_user_message(\"Fine, thanks.\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80bbdd",
   "metadata": {},
   "source": [
    "# Introduction to PromptTemplate\n",
    "\n",
    "The `PromptTemplate` is a powerful feature designed to streamline and standardize the creation of prompts for various applications, such as chatbots, automated responses, or data entry forms. It provides a structured format that can be reused across different scenarios, ensuring consistency and efficiency in how inputs are solicited and processed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98116ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giumast/miniforge3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# dynamic template and use of a Memory Buffer\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "Conversation:\n",
    "{chat}\n",
    "\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a43fc824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAct as a data scientist answering to every question with references to the beauty of Data Science.\n",
      "Conversation:\n",
      "\n",
      "\n",
      "New question: Hello, i lake the orange color.\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hello, i lake the orange color.',\n",
       " 'chat': '',\n",
       " 'text': \"Orange color is indeed beautiful! Did you know that data science can also be beautiful in the way it uncovers patterns and insights from complex datasets? Just like how different shades of orange can create a stunning visual impact, the analysis and visualization of data in data science can bring out the beauty of information hidden within the numbers. Let's explore the beauty of data science together!\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"Hello, i lake the orange color.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08fba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, i lake the orange color.\n",
      "AI: Orange color is indeed beautiful! Did you know that data science can also be beautiful in the way it uncovers patterns and insights from complex datasets? Just like how different shades of orange can create a stunning visual impact, the analysis and visualization of data in data science can bring out the beauty of information hidden within the numbers. Let's explore the beauty of data science together!\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9961a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAct as a data scientist answering to every question with references to the beauty of Data Science.\n",
      "Conversation:\n",
      "Human: Hello, i lake the orange color.\n",
      "AI: Orange color is indeed beautiful! Did you know that data science can also be beautiful in the way it uncovers patterns and insights from complex datasets? Just like how different shades of orange can create a stunning visual impact, the analysis and visualization of data in data science can bring out the beauty of information hidden within the numbers. Let's explore the beauty of data science together!\n",
      "\n",
      "New question: Tell me 3 fruits of my favourite color\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Tell me 3 fruits of my favourite color',\n",
       " 'chat': \"Human: Hello, i lake the orange color.\\nAI: Orange color is indeed beautiful! Did you know that data science can also be beautiful in the way it uncovers patterns and insights from complex datasets? Just like how different shades of orange can create a stunning visual impact, the analysis and visualization of data in data science can bring out the beauty of information hidden within the numbers. Let's explore the beauty of data science together!\",\n",
       " 'text': \"Sure! The color orange is associated with vibrant and bold fruits such as oranges, apricots, and persimmons. Just like how these fruits bring a burst of color and flavor to your palate, data science can bring a burst of insights and knowledge from analyzing data. The beauty of data science lies in its ability to uncover valuable information and trends that can help drive decision-making and innovation. Let's continue to appreciate the beauty of data science together!\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke({\"question\": \"Tell me 3 fruits of my favourite color\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33227adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, i lake the orange color.\n",
      "AI: Orange color is indeed beautiful! Did you know that data science can also be beautiful in the way it uncovers patterns and insights from complex datasets? Just like how different shades of orange can create a stunning visual impact, the analysis and visualization of data in data science can bring out the beauty of information hidden within the numbers. Let's explore the beauty of data science together!\n",
      "Human: Tell me 3 fruits of my favourite color\n",
      "AI: Sure! The color orange is associated with vibrant and bold fruits such as oranges, apricots, and persimmons. Just like how these fruits bring a burst of color and flavor to your palate, data science can bring a burst of insights and knowledge from analyzing data. The beauty of data science lies in its ability to uncover valuable information and trends that can help drive decision-making and innovation. Let's continue to appreciate the beauty of data science together!\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['chat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1197984",
   "metadata": {},
   "source": [
    "## LLM output parsing\n",
    "\n",
    "<a href=\"https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/quick_start/\" target=\"_blank\">source</a>\n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n",
    "\n",
    "**Output parsers** are classes that help *structure language model responses*. \n",
    "\n",
    "There are two main methods an output parser must implement:\n",
    "\n",
    "- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fe3e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Il colore arancione è meraviglioso, così come la bellezza dei dati nel campo della scienza. La data science ci permette di esplorare e analizzare i dati in modi sorprendenti, rivelando nuove informazioni e tendenze che altrimenti potrebbero sfuggire. È affascinante vedere come i dati possano raccontare storie e guidare decisioni importanti. La data science è una vera forma d'arte che porta luce e comprensione nel mondo dei dati.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OutputParser\n",
    "\n",
    "template = \"\"\"Act as a data scientist answering to every question with references to the beauty of Data Science.\n",
    "New question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"Mi piace il colore arancione\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ee8f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"id\": {\"title\": \"Id\", \"description\": \"user identification number\", \"type\": \"integer\"}, \"name\": {\"title\": \"Name\", \"description\": \"user name\", \"type\": \"string\"}, \"mail\": {\"title\": \"Mail\", \"description\": \"user mail address\", \"type\": \"string\"}}, \"required\": [\"id\", \"name\", \"mail\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int = Field(description=\"user identification number\")\n",
    "    name: str = Field(description=\"user name\")\n",
    "    mail: str = Field(description=\"user mail address\")\n",
    "    \n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=User)\n",
    "\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5d9fbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"id\": {\"title\": \"Id\", \"description\": \"user identification number\", \"type\": \"integer\"}, \"name\": {\"title\": \"Name\", \"description\": \"user name\", \"type\": \"string\"}, \"mail\": {\"title\": \"Mail\", \"description\": \"user mail address\", \"type\": \"string\"}}, \"required\": [\"id\", \"name\", \"mail\"]}\\n```'}, template='Analizza il testo\\n{format_instructions}\\n{query}\\n')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Analizza il testo\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94e8e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=1, name='John Doe', mail='john.doe@johndoe.com')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "query = \"id:1, name: John Doe, e-mail: john.doe@johndoe.com\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db6f408d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User(id=500, name='giuseppe mastrandrea', mail='mastrandreagiuseppe@gmail.com')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "query = \"my name is giuseppe mastrandrea, my email is mastrandreagiuseppe@gmail.com and my id is 500\"\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b379e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 500,\n",
       " 'name': 'giuseppe mastrandrea',\n",
       " 'mail': 'mastrandreagiuseppe@gmail.com'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "json_parser = JsonOutputParser(pydantic_object=User)\n",
    "\n",
    "chain = prompt | llm | json_parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
